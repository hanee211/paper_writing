

서론 




관련 연구

방향 및 연구 논문 선택

RNN


Recurrent Neural Networks Encoder-Decoder



서론



본 논문의 구성은 다음과 같다. 2장에서는 제안 방법의 배경지식인 RNN 및 RNN Encoder-Decoder 에 대해서 알아보고 또한 언어 생성 모델에 대해서도 살펴본다. 
3장에서는 본 논문에서 제안하는 문맥의 흐름에 따른 언어 학습 및 생성 모델에 대해서 설명하고, 4장에서는 제안 방법의 실험 방법 및 실험 결과를 기술한다. 마지막 5장에서는 결론 및 향후 연구에 대해서 서술한다. 




Recurrent Neural Network 을 응용하는 방법은 다양하게 연구되어 왔으며 그 중에서도 순차적인 데이터를 처리하는데
특히 많이 사용되어 왔다. 

순차적인 데이터를 생성하는데도 응용해 왔지만, 단어 사이의 일반적인 규칙에 한해 생성된다는 한계가 있다. 
비록 단어의 규칙과 발생 빈도 횟수 등에 의해 문장, 다음 단어가 예측되기 때문에 문장 같아 보이지만 
의미를 담고, 플롯으로 인한 글쓰기는 제한적이었음. 

컴퓨터에 의한 글쓰기는 글의 흐름을 담기에 제한적임. 
본 논문은 컴퓨터에 의한 주제 흐름에 따른 글쓰기 방법을 제안한다. 

주제 흐름을 구축하기 위한 모델을 생성하기 위해서 먼저 인코더와 디코더를 통해 문장의 공유한 벡터값을 추출한 후, 벡터값을 클러스터링을 하여 유사한
문장 벡터 값들로 재 구성한다. 본 문장을 다시 클러스터링 결과 값으로 치환한 뒤, 각 문서의 클러스터링 값의 의한 흐름을 재 학습한다. 
그 후 클러스터링의 흐름과 이전 문장과의 관계를 재 학습 한 뒤, 이 결과를 이용하여 문장을 새롭게 생성하는데 이용한다. 

제언된 기법의 정확도를 판변하기 위해서 ~~


>>> 인공 신경망, 딥러닝 발달, 텍스트에 내재된 의미 분석, 벡터 공간,
자연어 분석 -> 특히 자연어 분석, 기존의 규칙 기반, 확률 기반보다 용이하게 접근 가능

스토리 텔링, 기계 번역 등의 한계


키워드 : 문장 자동 생성, 순환 인공신경망, 클러스터링, 문장 고유 벡터, 주제에 맞는 글쓰기 
키워드 2: 문서 주제, 글의 문맥, 언어 학습 모델, Encoder-Decode,  seq2seq 모델, 문장 생성



언어 생성 및 학습 모델은 

일반적인 패턴., 스토리 텔링


벡터를 이용


스토리 텔링

플롯을 이용한 글쓰기

컴퓨터가 하기는 어렵다. 
컴퓨터의 글쓰기. 

RNN 문장의 시퀀스를 학습하여 


2. 관련 연구
문장 생성을 위해 많이 쓰이는 딥러닝 연구로는 RNN(Recurrent Neural Netowkr) 및 RNN을 응용한 RNN Encoder-Decoder과 같은 신경망 모델이 있다. RNN은 순환적인 구조를 가지는 인공 신경망이다. 모든 입력과 출력이 독립적으로 동작하는 기존 신경망과는 다르게 내부에 현재 상태를 저장하는 공간을 갖고 있어서 음성 인식과 같은 순차적인 데이터를 처리하는데 유리하다. RNN 의 구조적인 한계를 극복하고 좀 더 유연한 순차적 데이터를 처리하기 위한 RNN Encoder-Decoder은

문제가 있다. vanshing problem -> 이것을 해결 한 논문 정리필요 -> LSTM


Recurrent Neural Networks Encoder-Decoder
RNN Encoder-Decoder 모델은 기계 번역과 같은 특수한 경우를 위해서 RNN 두개를 이용한 모델이다. 일반 RNN 모델은 입력과 출력의 길이가 같아야 한다는 한계가 있다.
즉 다른 두 언어를 번역할 때 각 언어마다 필요한 단어의 수가 다르기 떄문에 일반적인 RNN 을 사용하여 기계 학습을 구현하는 것은 매우 어려운 일이다. 그러나 RNN Encoder-Encoder은 이 문제를 해결했으며, 문장의 길이가 다른 서로 다른 언어를 번역할 수 있게 한다[]. 
문장이 인코더에 입력되면, 임의의 길이 입력은 n-차원의 고정된 길이의 벡터로 인코딩이 수행된다. 인코더에서 출력된 벡터는 디코더의 초기값으로 사용된다. 디코더는 초기 
입력과 인코더에서 나온 출력 벡터를 초기 상태값으로 하여 출력 문장을 생성한다. 

LSTM  -encoder-decoder 구조, seq2seq 구조
문장의 고유한 특성을 나타내는 벡터를 추출하기 위해서 인코더 디코더 구조 사용, 기계 번역 및 chatbot 등 다양하게 사용 가능
수 식 



기본적인 인공 신경망 구조 이외에도 주제의 흐름이나 특정 목적을 갖고 문장을 생성하는 연구는 다양하게 진행되어 왔다. 텍스트의 포함되어 있는 의미를 파악하기 위해서 단어 및 문장을 벡터 공간에 사상하여 이를 활용하는 연구가 진행되어 왔다. 하지만 대부분 연구에서는 일반적인 단어의 패턴을 학습하여 이를 활용하는 방안으로 진행되어 왔기 때문에 특정한 목적의 글쓰기에서는 취약한 모습을 보였다. []에서는 일반적인 단어 패턴 뿐 아니라, 문맥(context)를 추가하여 학습 및 생성을 하는데 활용하고자 하였다. [] 에서는 문서 주제에 따라 다른 문장을 생성하는 LSTM 기반 언어 학습 모델을 제시하였다. 주어진 텍스트의 주제 및 의도를 고려하기 어려운 한계를 극복하기 위해 기존 LSTM 모델을 변형하여 문서의 주제와 주제안의 단어가 가지는 의미를 고려하여 그 특성이 벡터에 반영될 수 있도록 모델을 제시하였다. 그래서 그 결과로 문서의 주제를 고려한 문장을 생성할 수 있도록 하였다. [] 에서는 스토리 텔링을 위해 두 가지 모델을 생성하여 



문서 주제에 따른 문장 생성을 위한 LSTM 기반 언어 학습 모델




3. Proposed Method
본 장에서는 

문장 생성을 위한, 인코더-디코더 및 RNN 기반 언어 학습 및 생성 모델에 대해서 기술한다. 

기본적인 아이디어는 글을 일정한 플롯을 가지고, 그 플롯에 순서에 따라 맞는 문장을 기술한다. 
즉 어떠한 글에는 일정한 문맥이 있고 이 문맥에 기반하여 일련이 문장이 생성된다고 생각할 수 있다. 
따라서 본 논문에서는 주어진 글들에 대해서 문맥을 파악하고 그 문맥에 따른 문장 생성 패턴을 학습한 뒤
실제로 이 학습에 근거하여 문장을 생성하는 모델을 기반으로 연구를 진행하였다. 

모델 생성을 위해서 다음과 같은 단계를 거친다. 

1) 인코더 디코드를 이용한 문장 벡터 생성 모델
Cho et al., 2014 에서 소개된 기본적인 시퀀스 투 시퀀스을 사용한다. 시퀀스 투 시퀀스 모델은 각각 인코더 부분과 디코더 부분으로 구성되며
인코더 부분은 입력을 처리하고, 디코더 부분은 결과를 생성하는 모델이다. 

입력과 출력은 같은 문장으로 이 모델을 통해서 오토 인코더 같은 효과를 나타내며 문제 벡터를 생성할 수 있다. 

인코더에 입력 문장을 넣으면 RNN 구조를 통과하며 상태값과 결과를 출력한다. 상태값은 다시 디코더 부분의 초기값으로 입력이 되며
디코더의 결과가 다시 입력한 문장과 동일한 문장이 되로록 시퀀스 투 시퀀스 모델을 학습한다. 다만 디코더 부분은 학습의 편의성을 위해서
입력으로 원래 문장을 사용하며 학습이 끝나고 테스트 시에는 디코더의 출력이 다시 입력이 되도록 구성한다. 

시퀀스 투 시퀕스 모델의 학습을 통해서 인코더 부분과 디코더 부분 사이에 전달되는 인코더 부분의 마지막 상태값이 각 문장의 문장 벡터이다. 
새로운 문장이 왔을 때 인코더 부분을 통과하여 문장 벡터를 생성할 수 있으며, 문장 벡터를 디코더에 넣으면 본 문장을 다시 생성할 수 있다. 




2) 문장 벡터는 시퀀스 투 시퀀스 모델의 인코더 디코더를 통해서 생성되었고, 비슷한 의미와 구조를 가지는 문장들은 유사한 문장 벡터를 갖는다고 가정하였다. 두 문장간의 유사도를 측정하는 연구는 텍스트 마아닝, 문장 해석 및 문서 요약 등 필요한 분야이고, 연구가 활발히 진행되고 있는 분야이다. 방법이 다양하고, 복잡하기 때문에 도전적인 문제중 하나이다.
워드 임베딩 기법에 근거하여 두 문장간의 유사도를 측정하는 방법은 여러 연구에서 진행되었다. [][]. 이미 word2vec [] 같은 기법은 단어의 의미적 유성을 파악하는데 
유용하게 사용된다. 워드 임베딩은 문장 안에서의 단어의 위치와 주변 단어들 같에 관계를 이용하여 단어가 문장과 글에서 가지는 의미를 벡터 형식으로 효과적으로 표현해 왔다. 
따라서 단어 상위의 구성 요소, 즉 문장, 문단, 혹은 글의 의미와 유사도 등 자연어 처리 연구에 필요한 많은 연구에서 워드 임베딩을 이용하여 연구되었다.

워드 임베딩을 사용하여 문장의 의미를 파악하는 방법은 1)벡터 합, 2)벡터 곱, 3)문장을 구성하는 단어들의 선형 연산 등이 있고, 벡터 합과 벡터 곱은 그 방법이 용이하나
단어들의 선형 연산은 까다롭다[]. 단어들이 문장을 구성하는 방법이 여러가지가 있고, 




3) 문장 군집화
문장 벡터들을 k-means 클러스터링 기법을 사용하여 분류하였다. 
시퀀스 투 시퀀스 모델을 통해서 생성된 문장 벡터들은 유사한 의미의 문장이면 유사한 벡터 값을 갖는다. 즉 문장 벡터의 값들의 유사도를 측정하여 그 값이 유사하면
비슷한 문맥과 의미를 갖는 문장이라고 가정할 수 있다. 문장 벡터를 생성하는데 사용되는 워드 임베딩은 유사한 단어는 유사한 벡터 값을 갖도록 설정되었기 때문에 의미가 유사한
문장 벡터를 갖는 문장들은 서로 그 의미가 유사하다고 생각할 수 있다. 본 논문에서는 각 문서에서 포함하고 있는 문맥의 흐름을 이용하기 위하여 K-means 클러스터링 기법을 사용하여 데이터의 문장 벡터들을 약 1000여개로 그룹핑한다. K-mean 클러스터 알고리즘은 주어진 데이터를 여러 그룹으로 나누는 방법이다. 즉 n개의 입력 데이터를 받으면 n개의 데이터를 k개의 그룹으로 나누며, 각 군집은 클러스터를 형성한다. 즉 데이터를 한 개 이상의 데이터로 구성된 k개의 그룹으로 나누는 것이다. 각 그룹을 나누는 방법은 각 그룹안의 데이터 사이의 유사도 값이 최대가 되거나 비유사도가 최소가 되는 방법으로 이루어진다. 유사도를 측정하기 위해 데이터 사이의 거리의 제곱합을 최소로 하는 등, 비용 함수를 정의해야 한다. K-mean 클러스터 알고리즘은 각 그룹의 중심을 임의로 정한 뒤, 그룹내의 데이터들과 임의의 중심점이 최소가 되도록 하며, 이를 위해 반복적으로 그룹내의 중심점을 갱신하며, 그 결과가 최소가 되로록 한다.


본 연구에서는 문장의 유사도를 측정하기 위해서 각 문장 벡터의 값들의 차가 최소로 하는 비용 함수를 사용하였다. 즉 문장들의 클러스터를 구성하기 위해서 문장 벡터의 값을 최소로
하는 데이터들을 클러스트로 구성하여 사용하였다. 
하나의 문서, 글에는 다양한 문장의 있지만, 문서를 전체적으로  볼 때 유사한 의미를 갖는 문장이 다수 존재할 것이다. 유사한 의미를 갖는 다수의 문장을 하나의 클러스터로 분류하였고, 여기서 클러스터는 하나의 의미라고 가정할 수 있다. 하나의 문서나 글에는 일정한 흐름이 존재하고 본 연구에서는 그 흐름을 유사한 문장들이 구성하는 클러스터의 흐름으로 파악하여 분석하였다. 즉 하나의 문서 및 글의 문맥을 파악하는 방법은 그 글을 구성하는 문장들의 클러스터들의 순서로 파악하였다. 


4) 스토리 flow 학습 모델
클러스터링 기법으로 분류된 문장들은 다시 원래 문장으로 치환하여 원래의 문서가 문장의 흐름이 아닌 문맥, 즉 일련의 클러스터 흐름으로 나타내었다. 
즉 각 문장들은 각자 자신의 문장 벡터로 표현되며, 각 문장 벡터는 K-means 클러스터링을 통해 군집화되었으므로 각 문장은 클러스터링 값 중 하나를 갖게 된다. 
그러므로 각 문서들은 클러스터의 흐름을 갖게 되고, 이 클러스터링의 흐름을 스토리 flow 모델이라고 부를 수 있다. 



5) 스토리 생성 모델
스토리 생성 모델은 문맥의 클러스터 정보와 이전 문장의 정보를 이용하여 다음 문장을 생성한다.


문맥의 흐름에 맞는 문장 생성 패턴을 학습하기 위해서, LSTM 기반 언어 학습 모델에 사용하며, 문맥에 흐름에 따른 글쓰기 목적에 맞도록 이전 문장과 문맥의 정보를 반영한다. 
이전 문장과 문맥의 정보는 기존 LSTM 기반 언어 학습 모델의 입력에 추가로 덧붙여서 입력으로 반영된다. 즉 기존 LSTM 기반 언어 학습 모델이 현재
단어를 입력으로 하고, 다음 단어를 출력으로 한 반면, 본 논문에서 제시하는 모델은 이전 문장 정보와 문맥 정보 그리고 현재 단어를 입력으로 하고
다음 단어를 출력으로 하는 입 출력 구조를 가진다.

입력과 출력 데이터를 정리하면 다음과 같다. 현재 단어와 출력 단어는 워드 임베딩을 통해 사상된 벡터 값을 가진다. 그리고 이전 문장 정보는 시퀀스 투 시퀀스 모델을 통해서 생성된 문장 벡터 값이다. 마지막으로 문맥 정보는, 모든 문장 벡터 값들을 클러스터링을 통해서 생성된 클러스터 클래스를 의미하며, 여기서 사용되는 값은 각 클러스터들의 중심값으로 
여기서는 현재 문장이 포함되어 있는 클러스터의 중간 값이 된다. 입력은 현재 단어 벡터, 이전 문장의 벡터 마지막으로 현재 문장이 포함되는 클러스트의 중간값을 일렬로 접합한 
값이며, 출력은 다음 단어 벡터 값이 된다. 



수 식 필요

p(w|w-)

문장 생성 패턴 학습을 위해 세 가지 유형의 입력을 이용한다. 
첫번째 입력은 





IV Experiment
1. 실험 데이터 및 구성
본 실험의 데이터는 Document Understaning Conference(DUC)의 DUC 2002 데이터 셋을 사용한다. DUC 2002 데이터 셋은 여러개의 주제에 대한 여러 다중 문서를 포함하고 있으며 각 문서는 다수의 문장을 포함하고 있다. 약 60여개의 주제를 포함하고 있으며 각 주제별로 약 5~10개의 문서를 포함하고 있다. 각 데이터 셋의 세부적인 사항은 아래 표와 같다.



Short Text Similarity with Word Embeddings
Tom Kenter tom.kenter@uva.nl
Maarten de Rijke  derijke@uva.nl

A Word Embeddings Model for Sentence Similarity
Victor Mijangos, Gerardo Sierra and Abel Herrera

LSTM based Language Model for Topic-focused Sentence Generation
Dahae Kim, 

Recurrent Neural Networks for Storytelling.

